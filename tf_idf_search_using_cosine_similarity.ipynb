{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable Tensorflow warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import json\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a Spacy Language model\n",
    "sp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open('data/summaries.json', 'r') as outfile:\n",
    "    summaries = json.load(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a corpus vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all tokenized texts into a single list\n",
    "tokenized_texts = [i[\"tokenized_text\"] for i in summaries]\n",
    "\n",
    "# flatten the list of lists\n",
    "vocab = list(itertools.chain(*tokenized_texts))\n",
    "\n",
    "# remove duplicates\n",
    "vocab = list(set(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vocabulary\n",
    "with open('data/vocab.json', 'w') as outfile:\n",
    "    json.dump(vocab, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count how many times each token occurs in a document\n",
    "docs_token_counter = []\n",
    "for doc in summaries:\n",
    "    doc_tokenized = doc[\"tokenized_text\"]\n",
    "    docs_token_counter.append(Counter(doc_tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find all unique tokens in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each token in corpus vocabulary, count in how many documents it occurs\n",
    "number_docs_with_token  = {}\n",
    "for token in vocab:\n",
    "    count_docs = sum([1 for doc in docs_token_counter if token in doc.keys()])\n",
    "    number_docs_with_token[token] = count_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_docs_with_token['ebola']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute TfIdfs of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(docs_token_counter):\n",
    "    doc_length = len(doc)\n",
    "    tfidf_vec = []\n",
    "    for token in vocab:\n",
    "        \n",
    "        # compute a term frequency (tf) per document\n",
    "        tf = doc[token] / len(summaries[i][\"tokenized_text\"])\n",
    "        \n",
    "        # compute a log of inverse document frequency per document\n",
    "\n",
    "        idf = np.log(len(summaries)/number_docs_with_token[token])\n",
    "\n",
    "        tfidf = tf * idf\n",
    "        tfidf_vec.append(tfidf)\n",
    "    \n",
    "    # add tf_idf vector to the dictionaries\n",
    "    summaries[i]['tf_idf'] = tfidf_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save an updates summary with computed Tf-Idf vectors\n",
    "with open('data/summaries.json', 'w') as json_file:\n",
    "    json.dump(summaries, json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"highest pandemic casualties\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse the tokenizer from Milestone 1 to tokenize search queries\n",
    "\n",
    "def tokenizer(document):\n",
    "    text_lowercased = sp(document.lower())\n",
    "    tokens_without_stopwords = [word for word \n",
    "                     in text_lowercased \n",
    "                     if not word.is_stop \n",
    "                     and not word.is_punct\n",
    "                     and len(word.dep_.strip())!=0]   \n",
    "    \n",
    "    token_lemmatized = [token.lemma_ \n",
    "               for token\n",
    "               in tokens_without_stopwords]\n",
    "    \n",
    "    return token_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reuse the workflow for article Tf-Idf calculation\n",
    "# to build a vectorizer function for search queries\n",
    "\n",
    "def vectorize(query, vocab = vocab):\n",
    "    \n",
    "    query_tokenized = tokenizer(query)\n",
    "    query_token_counter = Counter(query_tokenized)\n",
    "    query_vec = []\n",
    "    for token in vocab:\n",
    "        \n",
    "        tf = query_token_counter[token] / len(query_tokenized)\n",
    "        idf = np.log(len(summaries) /  number_docs_with_token[token])\n",
    "        tfidf = tf * idf\n",
    "        query_vec.append(tfidf)\n",
    "            \n",
    "    return query_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search documents with Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a search function\n",
    "def search_tfidf(query, docs):\n",
    "    \n",
    "    # vectorize query\n",
    "    query_vec = vectorize(query)\n",
    "    query_arr = np.array(query_vec)\n",
    "    \n",
    "    # Build a list of results and their cosine similarity scores\n",
    "    rankings = []\n",
    "    for doc in docs:\n",
    "        doc_rank = {}\n",
    "        doc_arr = np.array(doc['tf_idf'])\n",
    "        rank = cosine_similarity(query_arr.reshape(1,-1), doc_arr.reshape(1, -1))[0][0]\n",
    "        if rank > 0:\n",
    "            doc_rank['title'] = doc['title']\n",
    "            doc_rank['rank'] = rank\n",
    "            rankings.append(doc_rank)\n",
    "\n",
    "    #return sorted results\n",
    "    return sorted(rankings, key=lambda k: k['rank'], reverse=True)\n",
    "ranking = search_tfidf(query, summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Plague of Cyprian', 'rank': 0.11768601854662974},\n",
       " {'title': 'Science diplomacy and pandemics', 'rank': 0.07115769346687584}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_tfidf(\"ebola\", summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Plague of Cyprian was a pandemic that afflicted the Roman Empire about from AD 249 to 262. The plague is thought to have caused widespread manpower shortages for food production and the Roman army, severely weakening the empire during the Crisis of the Third Century. Its modern name commemorates St. Cyprian, bishop of Carthage, an early Christian writer who witnessed and described the plague. The agent of the plague is highly speculative because of sparse sourcing, but suspects have included smallpox, pandemic influenza and viral hemorrhagic fever (filoviruses) like the Ebola virus.\n"
     ]
    }
   ],
   "source": [
    "# Lets check if the article 'Plague of Cyprian' has a word \"ebola\" in it\n",
    "for s in summaries:\n",
    "    if s[\"title\"] == 'Plague of Cyprian':\n",
    "        print(s[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
