{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable Tensorflow warnings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or any {'0', '1', '2'}\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "import json\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a Spacy Language model\n",
    "sp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['title', 'text', 'url'])\n"
     ]
    }
   ],
   "source": [
    "## assuming that the data.json is copied to a data folder\n",
    "with open('data/data.json', 'r') as outfile:\n",
    "    summaries = json.load(outfile)\n",
    "print(summaries[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercase and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = summaries[1][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'> hiv PROPN nmod\n",
      "<class 'spacy.tokens.token.Token'> / SYM punct\n",
      "<class 'spacy.tokens.token.Token'> aids PROPN nsubjpass\n",
      "<class 'spacy.tokens.token.Token'> , PUNCT punct\n",
      "<class 'spacy.tokens.token.Token'> or CCONJ cc\n"
     ]
    }
   ],
   "source": [
    "# Lowercase data. Lowercase the text. \n",
    "# Explore the attributes of each token returned SpaCy.\n",
    "text_tokenized = sp(text.lower())\n",
    "for token in text_tokenized[:5]:\n",
    "    print(type(token), token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for token in text_tokenized:\n",
    "#     print(token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## we can find out if the tokenizer was unable to identify some of the tokens\n",
    "unclassified_tokens = [(token.lemma_, token.dep_) \n",
    "                       for token \n",
    "                       in text_tokenized \n",
    "                       if token.dep_ == \"\"]\n",
    "\n",
    "# Tokens like these are not useful to search, we will remove them in the next step:\n",
    "unclassified_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[hiv,\n",
       " aids,\n",
       " human,\n",
       " immunodeficiency,\n",
       " virus,\n",
       " considered,\n",
       " authors,\n",
       " global,\n",
       " pandemic,\n",
       " currently]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_without_sw = [word for word \n",
    "                     in text_tokenized \n",
    "                     if not word.is_stop \n",
    "                     and not word.is_punct\n",
    "                    ]\n",
    "tokens_without_sw[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize (tokenize) the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hiv',\n",
       " 'aids',\n",
       " 'human',\n",
       " 'immunodeficiency',\n",
       " 'virus',\n",
       " 'consider',\n",
       " 'author',\n",
       " 'global',\n",
       " 'pandemic',\n",
       " 'currently']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_lemmas = [token.lemma_ \n",
    "               for token\n",
    "               in tokens_without_sw\n",
    "               if token.dep_]\n",
    "token_lemmas[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(document):\n",
    "    text_lowercased = sp(document.lower())\n",
    "    tokens_without_stopwords = [word \n",
    "                                for word \n",
    "                                in text_lowercased\n",
    "                                if not word.is_stop \n",
    "                                and not word.is_punct]\n",
    "    \n",
    "    token_lemmatized = [token.lemma_ \n",
    "               for token\n",
    "               in tokens_without_stopwords\n",
    "               if token.dep_]\n",
    "    \n",
    "    return token_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in summaries:\n",
    "    doc['tokenized_text'] = tokenizer(doc['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pandemic',\n",
       " 'greek',\n",
       " 'πᾶν',\n",
       " 'pan',\n",
       " 'δῆμος',\n",
       " 'demos',\n",
       " 'people',\n",
       " 'epidemic',\n",
       " 'infectious',\n",
       " 'disease',\n",
       " 'spread',\n",
       " 'large',\n",
       " 'region',\n",
       " 'instance',\n",
       " 'multiple',\n",
       " 'continent',\n",
       " 'worldwide',\n",
       " 'affect',\n",
       " 'substantial',\n",
       " 'number',\n",
       " 'people',\n",
       " 'widespread',\n",
       " 'endemic',\n",
       " 'disease',\n",
       " 'stable',\n",
       " 'number',\n",
       " 'infected',\n",
       " 'people',\n",
       " 'pandemic',\n",
       " 'widespread',\n",
       " 'endemic',\n",
       " 'disease',\n",
       " 'stable',\n",
       " 'number',\n",
       " 'infected',\n",
       " 'people',\n",
       " 'recurrence',\n",
       " 'seasonal',\n",
       " 'influenza',\n",
       " 'generally',\n",
       " 'exclude',\n",
       " 'occur',\n",
       " 'simultaneously',\n",
       " 'large',\n",
       " 'region',\n",
       " 'globe',\n",
       " 'spread',\n",
       " 'worldwide',\n",
       " '\\n',\n",
       " 'human',\n",
       " 'history',\n",
       " 'number',\n",
       " 'pandemic',\n",
       " 'disease',\n",
       " 'smallpox',\n",
       " 'tuberculosis',\n",
       " 'fatal',\n",
       " 'pandemic',\n",
       " 'recorded',\n",
       " 'history',\n",
       " 'black',\n",
       " 'death',\n",
       " 'know',\n",
       " 'plague',\n",
       " 'kill',\n",
       " 'estimate',\n",
       " '75–200',\n",
       " 'million',\n",
       " 'people',\n",
       " '14th',\n",
       " 'century',\n",
       " 'term',\n",
       " 'later',\n",
       " 'pandemic',\n",
       " 'include',\n",
       " '1918',\n",
       " 'influenza',\n",
       " 'pandemic',\n",
       " 'spanish',\n",
       " 'flu',\n",
       " 'current',\n",
       " 'pandemic',\n",
       " 'include',\n",
       " 'covid-19',\n",
       " 'sars',\n",
       " 'cov-2',\n",
       " 'hiv',\n",
       " 'aids']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets take a look what our tokenized summaries look like:\n",
    "summaries[0]['tokenized_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenized texts to file:\n",
    "with open('data/summaries.json', 'w') as outfile:\n",
    "    json.dump(summaries, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml] *",
   "language": "python",
   "name": "conda-env-ml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
